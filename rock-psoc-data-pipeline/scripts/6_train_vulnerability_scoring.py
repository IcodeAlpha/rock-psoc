"""
Train Vulnerability Risk Scoring Models (CISA KEV Dataset)
Script: 6_train_vulnerability_scoring.py

Tests 3 algorithms: Random Forest, XGBoost, and Neural Network
Two tasks: Regression (risk score 0-100) and Classification (Low/Med/High severity)
"""

import pandas as pd
import json
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from xgboost import XGBRegressor, XGBClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    r2_score,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    classification_report,
    confusion_matrix
)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import joblib
import os
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("VULNERABILITY RISK SCORING MODEL TRAINING (CISA KEV)")
print("=" * 70)

# ============================================
# 1. LOAD AND PREPARE CISA DATA
# ============================================
print("\n[1/7] Loading CISA KEV data...")

try:
    df = pd.read_csv('data/raw/cisa_kev.csv')
    print(f"‚úÖ Loaded {len(df)} vulnerability records")
    print(f"   Columns: {list(df.columns)}")
    
except FileNotFoundError:
    print("‚ùå Error: cisa_kev.csv not found!")
    print("   Run '1_download_datasets.py' first")
    exit(1)

# ============================================
# 2. FEATURE ENGINEERING
# ============================================
print("\n[2/7] Engineering features from CISA data...")

# Common CISA KEV columns
# cveID, vendorProject, product, vulnerabilityName, dateAdded, 
# shortDescription, requiredAction, dueDate, knownRansomwareCampaignUse

# Create temporal features
if 'dateAdded' in df.columns:
    df['dateAdded'] = pd.to_datetime(df['dateAdded'], errors='coerce')
    df['days_since_added'] = (pd.Timestamp.now() - df['dateAdded']).dt.days
    df['month_added'] = df['dateAdded'].dt.month
    df['year_added'] = df['dateAdded'].dt.year
    print("   ‚úÖ Created temporal features")

if 'dueDate' in df.columns:
    df['dueDate'] = pd.to_datetime(df['dueDate'], errors='coerce')
    df['has_due_date'] = df['dueDate'].notna().astype(int)
    df['days_until_due'] = (df['dueDate'] - pd.Timestamp.now()).dt.days
    df['days_until_due'] = df['days_until_due'].fillna(999)  # No due date = far future
    print("   ‚úÖ Created due date features")

# Text-based features
if 'shortDescription' in df.columns:
    df['description_length'] = df['shortDescription'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)
    df['description_words'] = df['shortDescription'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)
    print("   ‚úÖ Created description features")

if 'vulnerabilityName' in df.columns:
    df['vuln_name_length'] = df['vulnerabilityName'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)

# Ransomware indicator
if 'knownRansomwareCampaignUse' in df.columns:
    df['is_ransomware'] = df['knownRansomwareCampaignUse'].apply(
        lambda x: 1 if str(x).lower() == 'known' else 0
    )
    print("   ‚úÖ Created ransomware indicator")

# Encode categorical variables
categorical_features = {}

if 'vendorProject' in df.columns:
    vendor_encoder = LabelEncoder()
    df['vendor_encoded'] = vendor_encoder.fit_transform(df['vendorProject'].astype(str))
    categorical_features['vendor'] = vendor_encoder
    print(f"   ‚úÖ Encoded vendors: {len(vendor_encoder.classes_)} unique")

if 'product' in df.columns:
    product_encoder = LabelEncoder()
    df['product_encoded'] = product_encoder.fit_transform(df['product'].astype(str))
    categorical_features['product'] = product_encoder
    print(f"   ‚úÖ Encoded products: {len(product_encoder.classes_)} unique")

# ============================================
# 3. CREATE TARGET VARIABLES
# ============================================
print("\n[3/7] Creating target variables...")

# TARGET 1: RISK SCORE (Regression - 0 to 100)
# Create synthetic risk score based on urgency and characteristics
risk_components = []

# Component 1: Urgency (40 points max)
if 'has_due_date' in df.columns:
    urgency_score = df['has_due_date'] * 20
    days_score = np.clip(30 - (df['days_until_due'] / 10), 0, 20)
    risk_components.append(urgency_score + days_score)
else:
    risk_components.append(pd.Series([20] * len(df)))

# Component 2: Recency (30 points max)
if 'days_since_added' in df.columns:
    recency_score = np.clip(30 - (df['days_since_added'] / 20), 0, 30)
    risk_components.append(recency_score)
else:
    risk_components.append(pd.Series([15] * len(df)))

# Component 3: Ransomware (20 points)
if 'is_ransomware' in df.columns:
    risk_components.append(df['is_ransomware'] * 20)
else:
    risk_components.append(pd.Series([0] * len(df)))

# Component 4: Complexity (10 points)
if 'description_length' in df.columns:
    complexity_score = np.clip(df['description_length'] / 50, 0, 10)
    risk_components.append(complexity_score)
else:
    risk_components.append(pd.Series([5] * len(df)))

# Sum and normalize to 0-100
df['risk_score'] = sum(risk_components)
df['risk_score'] = np.clip(df['risk_score'], 0, 100)

print(f"   ‚úÖ Created risk_score (regression target)")
print(f"      Range: {df['risk_score'].min():.1f} - {df['risk_score'].max():.1f}")
print(f"      Mean: {df['risk_score'].mean():.1f}")

# TARGET 2: SEVERITY LEVEL (Classification - Low/Medium/High)
df['severity'] = pd.cut(
    df['risk_score'],
    bins=[0, 33, 66, 100],
    labels=['Low', 'Medium', 'High'],
    include_lowest=True
)

severity_encoder = LabelEncoder()
df['severity_encoded'] = severity_encoder.fit_transform(df['severity'])

print(f"   ‚úÖ Created severity (classification target)")
print(f"      Distribution:")
for sev, count in df['severity'].value_counts().sort_index().items():
    print(f"         {sev}: {count}")

# ============================================
# 4. SELECT FEATURES
# ============================================
print("\n[4/7] Selecting features...")

feature_cols = []

# Add all engineered features
numeric_features = [
    'vendor_encoded', 'product_encoded',
    'description_length', 'description_words',
    'vuln_name_length', 'days_since_added',
    'has_due_date', 'days_until_due',
    'is_ransomware', 'month_added', 'year_added'
]

for feat in numeric_features:
    if feat in df.columns:
        feature_cols.append(feat)

print(f"   ‚úÖ Selected {len(feature_cols)} features:")
for feat in feature_cols:
    print(f"      - {feat}")

# Prepare feature matrix
X = df[feature_cols].fillna(0)
y_regression = df['risk_score']
y_classification = df['severity_encoded']

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(f"\n   ‚úÖ Features scaled")

# ============================================
# PART A: REGRESSION MODELS (Risk Score)
# ============================================
print("\n" + "=" * 70)
print("PART A: REGRESSION - PREDICTING RISK SCORE (0-100)")
print("=" * 70)

# Split data for regression
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_scaled, y_regression,
    test_size=0.2,
    random_state=42
)

X_train_nn_reg, X_val_nn_reg, y_train_nn_reg, y_val_nn_reg = train_test_split(
    X_train_reg, y_train_reg,
    test_size=0.2,
    random_state=42
)

print(f"\n   Training samples: {len(X_train_reg)}")
print(f"   Test samples: {len(X_test_reg)}")

regression_results = {}

# --- RANDOM FOREST REGRESSOR ---
print("\n" + "-" * 70)
print("Model 1: Random Forest Regressor")
print("-" * 70)

rf_reg = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    random_state=42,
    n_jobs=-1
)

print("Training...")
rf_reg.fit(X_train_reg, y_train_reg)

rf_pred_reg = rf_reg.predict(X_test_reg)
rf_mse = mean_squared_error(y_test_reg, rf_pred_reg)
rf_mae = mean_absolute_error(y_test_reg, rf_pred_reg)
rf_r2 = r2_score(y_test_reg, rf_pred_reg)

regression_results['Random Forest'] = {
    'mse': rf_mse,
    'rmse': np.sqrt(rf_mse),
    'mae': rf_mae,
    'r2': rf_r2
}

print(f"‚úÖ Results:")
print(f"   MSE:  {rf_mse:.4f}")
print(f"   RMSE: {np.sqrt(rf_mse):.4f}")
print(f"   MAE:  {rf_mae:.4f}")
print(f"   R¬≤:   {rf_r2:.4f}")

# --- XGBOOST REGRESSOR ---
print("\n" + "-" * 70)
print("Model 2: XGBoost Regressor")
print("-" * 70)

xgb_reg = XGBRegressor(
    n_estimators=100,
    max_depth=8,
    learning_rate=0.1,
    random_state=42,
    verbosity=0
)

print("Training...")
xgb_reg.fit(X_train_reg, y_train_reg)

xgb_pred_reg = xgb_reg.predict(X_test_reg)
xgb_mse = mean_squared_error(y_test_reg, xgb_pred_reg)
xgb_mae = mean_absolute_error(y_test_reg, xgb_pred_reg)
xgb_r2 = r2_score(y_test_reg, xgb_pred_reg)

regression_results['XGBoost'] = {
    'mse': xgb_mse,
    'rmse': np.sqrt(xgb_mse),
    'mae': xgb_mae,
    'r2': xgb_r2
}

print(f"‚úÖ Results:")
print(f"   MSE:  {xgb_mse:.4f}")
print(f"   RMSE: {np.sqrt(xgb_mse):.4f}")
print(f"   MAE:  {xgb_mae:.4f}")
print(f"   R¬≤:   {xgb_r2:.4f}")

# --- NEURAL NETWORK REGRESSOR ---
print("\n" + "-" * 70)
print("Model 3: Neural Network Regressor")
print("-" * 70)

nn_reg = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train_reg.shape[1],)),
    layers.BatchNormalization(),
    layers.Dropout(0.2),
    
    layers.Dense(32, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.2),
    
    layers.Dense(16, activation='relu'),
    
    layers.Dense(1, activation='linear')  # Linear for regression
], name='VulnerabilityRiskNN')

nn_reg.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='mse',
    metrics=['mae']
)

print(f"Architecture: {X_train_reg.shape[1]} ‚Üí 64 ‚Üí 32 ‚Üí 16 ‚Üí 1")

early_stop = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True,
    verbose=0
)

print("Training...")
history_reg = nn_reg.fit(
    X_train_nn_reg, y_train_nn_reg,
    validation_data=(X_val_nn_reg, y_val_nn_reg),
    epochs=100,
    batch_size=64,
    callbacks=[early_stop],
    verbose=0
)

nn_pred_reg = nn_reg.predict(X_test_reg, verbose=0).flatten()
nn_mse = mean_squared_error(y_test_reg, nn_pred_reg)
nn_mae = mean_absolute_error(y_test_reg, nn_pred_reg)
nn_r2 = r2_score(y_test_reg, nn_pred_reg)

regression_results['Neural Network'] = {
    'mse': nn_mse,
    'rmse': np.sqrt(nn_mse),
    'mae': nn_mae,
    'r2': nn_r2
}

print(f"‚úÖ Results:")
print(f"   MSE:  {nn_mse:.4f}")
print(f"   RMSE: {np.sqrt(nn_mse):.4f}")
print(f"   MAE:  {nn_mae:.4f}")
print(f"   R¬≤:   {nn_r2:.4f}")

# ============================================
# PART B: CLASSIFICATION MODELS (Severity)
# ============================================
print("\n" + "=" * 70)
print("PART B: CLASSIFICATION - PREDICTING SEVERITY (Low/Med/High)")
print("=" * 70)

# Split data for classification
# Check if we can stratify
min_class_count_clf = pd.Series(y_classification).value_counts().min()
can_stratify_clf = min_class_count_clf >= 2

if can_stratify_clf:
    stratify_clf = y_classification
else:
    print(f"   ‚ö†Ô∏è  Cannot stratify (min class has {min_class_count_clf} samples)")
    stratify_clf = None

X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
    X_scaled, y_classification,
    test_size=0.2,
    random_state=42,
    stratify=stratify_clf
)

# Check for validation split
min_train_count_clf = pd.Series(y_train_clf).value_counts().min()
can_stratify_val_clf = min_train_count_clf >= 2

X_train_nn_clf, X_val_nn_clf, y_train_nn_clf, y_val_nn_clf = train_test_split(
    X_train_clf, y_train_clf,
    test_size=0.2,
    random_state=42,
    stratify=y_train_clf if can_stratify_val_clf else None
)

print(f"\n   Training samples: {len(X_train_clf)}")
print(f"   Test samples: {len(X_test_clf)}")

classification_results = {}

# --- RANDOM FOREST CLASSIFIER ---
print("\n" + "-" * 70)
print("Model 1: Random Forest Classifier")
print("-" * 70)

rf_clf = RandomForestClassifier(
    n_estimators=100,
    max_depth=15,
    random_state=42,
    n_jobs=-1,
    class_weight='balanced'
)

print("Training...")
rf_clf.fit(X_train_clf, y_train_clf)

rf_pred_clf = rf_clf.predict(X_test_clf)
rf_accuracy = accuracy_score(y_test_clf, rf_pred_clf)
rf_f1 = f1_score(y_test_clf, rf_pred_clf, average='weighted')

classification_results['Random Forest'] = {
    'accuracy': rf_accuracy,
    'f1_score': rf_f1
}

print(f"‚úÖ Results:")
print(f"   Accuracy: {rf_accuracy:.4f}")
print(f"   F1-Score: {rf_f1:.4f}")

# --- XGBOOST CLASSIFIER ---
print("\n" + "-" * 70)
print("Model 2: XGBoost Classifier")
print("-" * 70)

xgb_clf = XGBClassifier(
    n_estimators=100,
    max_depth=8,
    learning_rate=0.1,
    random_state=42,
    eval_metric='mlogloss',
    verbosity=0
)

print("Training...")
xgb_clf.fit(X_train_clf, y_train_clf)

xgb_pred_clf = xgb_clf.predict(X_test_clf)
xgb_accuracy = accuracy_score(y_test_clf, xgb_pred_clf)
xgb_f1 = f1_score(y_test_clf, xgb_pred_clf, average='weighted')

classification_results['XGBoost'] = {
    'accuracy': xgb_accuracy,
    'f1_score': xgb_f1
}

print(f"‚úÖ Results:")
print(f"   Accuracy: {xgb_accuracy:.4f}")
print(f"   F1-Score: {xgb_f1:.4f}")

# --- NEURAL NETWORK CLASSIFIER ---
print("\n" + "-" * 70)
print("Model 3: Neural Network Classifier")
print("-" * 70)

num_classes = len(severity_encoder.classes_)
y_train_cat = keras.utils.to_categorical(y_train_nn_clf, num_classes)
y_val_cat = keras.utils.to_categorical(y_val_nn_clf, num_classes)
y_test_cat = keras.utils.to_categorical(y_test_clf, num_classes)

nn_clf = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train_clf.shape[1],)),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    
    layers.Dense(32, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.2),
    
    layers.Dense(num_classes, activation='softmax')
], name='VulnerabilitySeverityNN')

nn_clf.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print(f"Architecture: {X_train_clf.shape[1]} ‚Üí 64 ‚Üí 32 ‚Üí {num_classes}")

print("Training...")
history_clf = nn_clf.fit(
    X_train_nn_clf, y_train_cat,
    validation_data=(X_val_nn_clf, y_val_cat),
    epochs=100,
    batch_size=64,
    callbacks=[early_stop],
    verbose=0
)

nn_pred_probs = nn_clf.predict(X_test_clf, verbose=0)
nn_pred_clf = np.argmax(nn_pred_probs, axis=1)
nn_accuracy = accuracy_score(y_test_clf, nn_pred_clf)
nn_f1 = f1_score(y_test_clf, nn_pred_clf, average='weighted')

classification_results['Neural Network'] = {
    'accuracy': nn_accuracy,
    'f1_score': nn_f1
}

print(f"‚úÖ Results:")
print(f"   Accuracy: {nn_accuracy:.4f}")
print(f"   F1-Score: {nn_f1:.4f}")

# ============================================
# 5. COMPARE RESULTS
# ============================================
print("\n" + "=" * 70)
print("RESULTS SUMMARY")
print("=" * 70)

print("\nüìä REGRESSION (Risk Score Prediction):")
reg_df = pd.DataFrame(regression_results).T
print(reg_df.to_string())
best_reg_model = min(regression_results, key=lambda x: regression_results[x]['mae'])
print(f"\nüèÜ Best Regression Model: {best_reg_model} (MAE: {regression_results[best_reg_model]['mae']:.4f})")

print("\nüìä CLASSIFICATION (Severity Prediction):")
clf_df = pd.DataFrame(classification_results).T
print(clf_df.to_string())
best_clf_model = max(classification_results, key=lambda x: classification_results[x]['f1_score'])
print(f"\nüèÜ Best Classification Model: {best_clf_model} (F1: {classification_results[best_clf_model]['f1_score']:.4f})")

# ============================================
# 6. SAVE MODELS
# ============================================
print("\n[7/7] Saving models...")

os.makedirs('models/saved_models/vulnerability_scoring', exist_ok=True)
os.makedirs('models/preprocessors', exist_ok=True)
os.makedirs('models/evaluation', exist_ok=True)

# Save regression models
joblib.dump(rf_reg, 'models/saved_models/vulnerability_scoring/rf_regressor.pkl')
joblib.dump(xgb_reg, 'models/saved_models/vulnerability_scoring/xgb_regressor.pkl')
nn_reg.save('models/saved_models/vulnerability_scoring/nn_regressor.h5')
print("‚úÖ Saved regression models")

# Save classification models
joblib.dump(rf_clf, 'models/saved_models/vulnerability_scoring/rf_classifier.pkl')
joblib.dump(xgb_clf, 'models/saved_models/vulnerability_scoring/xgb_classifier.pkl')
nn_clf.save('models/saved_models/vulnerability_scoring/nn_classifier.h5')
print("‚úÖ Saved classification models")

# Save preprocessors
joblib.dump(scaler, 'models/preprocessors/vulnerability_scaler.pkl')
joblib.dump(severity_encoder, 'models/preprocessors/severity_encoder.pkl')

for name, encoder in categorical_features.items():
    joblib.dump(encoder, f'models/preprocessors/{name}_encoder.pkl')

print("‚úÖ Saved preprocessors")

# Save feature names
with open('models/preprocessors/vulnerability_feature_names.json', 'w') as f:
    json.dump(feature_cols, f)

# Save metrics
evaluation_data = {
    'regression': regression_results,
    'classification': classification_results,
    'best_regression_model': best_reg_model,
    'best_classification_model': best_clf_model,
    'num_features': len(feature_cols),
    'severity_classes': list(severity_encoder.classes_)
}

with open('models/evaluation/vulnerability_scoring_metrics.json', 'w') as f:
    json.dump(evaluation_data, f, indent=4)

print("‚úÖ Saved evaluation metrics")

# ============================================
# FINAL SUMMARY
# ============================================
print("\n" + "=" * 70)
print("TRAINING COMPLETE!")
print("=" * 70)

print(f"\nüìä Summary:")
print(f"   Dataset: CISA Known Exploited Vulnerabilities")
print(f"   Samples: {len(df)}")
print(f"   Features: {len(feature_cols)}")
print(f"\n   Tasks:")
print(f"      1. Regression (Risk Score 0-100)")
print(f"         üèÜ Best: {best_reg_model} (MAE: {regression_results[best_reg_model]['mae']:.4f})")
print(f"      2. Classification (Low/Med/High)")
print(f"         üèÜ Best: {best_clf_model} (F1: {classification_results[best_clf_model]['f1_score']:.4f})")

print(f"\n   Models Trained: 6 total")
print(f"      ‚úì 3 Regression models")
print(f"      ‚úì 3 Classification models")

print(f"\nüìÅ Saved to:")
print(f"   models/saved_models/vulnerability_scoring/")
print(f"   models/preprocessors/")
print(f"   models/evaluation/")

print(f"\n‚úÖ Next Step: Run 7_evaluate_models.py")